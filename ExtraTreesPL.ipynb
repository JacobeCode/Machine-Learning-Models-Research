{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekt UM - badanie modeli i optymalizacji metod klasyfikacji\n",
    "### Część modelu - Las losowy w wariancie Extra Trees\n",
    "\n",
    " Wybrane modele do klasyfikacji to :\n",
    " - Las Losowy (w opcji Extra Trees) [wykonanie - Jakub Gucik]\n",
    " - Wektory Nośne [wykonanie - Tymoteusz Macewicz]\n",
    "\n",
    "Każdy z członków zespołu zajmie się jednym z modeli.\n",
    "\n",
    "Problem badawczy : Ze względów projektowych wstępną bazą Wykorzystaną do klasyfikacji był zestaw - 'Drum Kit Sound Samples':\n",
    "\n",
    "*https://www.kaggle.com/datasets/anubhavchhabra/drum-kit-sound-samples*\n",
    "\n",
    "W tym wypadku klasyfikowane były dźwięki perkusyjne w celu rozróżnienia np. bębnów czy werbli, a dokładnie zgodnie z angielską nomenklaturą bazy danych :\n",
    "- \"kick\"\n",
    "- \"snare\"\n",
    "- \"toms\"\n",
    "- \"overheads\"\n",
    "\n",
    " W bazie znajduje się dokładnie 160 sampli audio, gdzie każdy z rodzajów to 40 plików .wav. Nagrania zostały pozyskane drogą nagrań 'live' lub są to dźwięki 'symulowane' techniką komputerową. Bazę można wykorzystać również do zadań klasteryzacyjnych.\n",
    "\n",
    "Jednak przez wzgląd, iż wybrane modele na tej bazie danych domyślnie otrzymywały już około 100% skuteczności, skłonił do podjęcia wyboru zmiany danych. Najpewniejszym powodem takiego zachowania modeli jest po prostu zbyt mała i nie różnorodna baza danych. Nie chcąc też łączyć baz danych, by uniknąć problemów z processingiem czy niejednorodnością nagrań, zmieniono bazę na większą.\n",
    "\n",
    "Finalnie wybrano bazę – ‘IRMAS’, którą można znaleźć na stronie: \n",
    "\n",
    "https://www.upf.edu/web/mtg/irmas. \n",
    "\n",
    "Jest to zbiór danych w postaci plików .wav, przeznaczony do rozpoznawania instrumentów w sygnałach muzycznych audio – zawiera wiele sampli z oznaczonymi dominującymi instrumentami, czyli label’ami niezbędnymi do uczenia nadzorowanego. Jest więc to baza przeznaczona do zastosowań w automatycznym rozpoznawaniu i klasyfikacji. Zestaw danych został przygotowany przez Uniwersytet Pompeu Fabry w Barcelonie.\n",
    "Same pliki zawierają adnotacje dotyczące głównego dominującego instrumentu grającego, zgodnie z oznaczeniami:\n",
    "\n",
    "-   cel – wiolonczela [ang. Cello]\n",
    "-   cla – klarnet [ang. Clarinet]\n",
    "-   flu – flet [ang. Flute]\n",
    "-   gac – gitara akustyczna [ang. Acoustic Guitar]\n",
    "-   gel – gitara elektryczna [ang. Electric Guitar]\n",
    "-   org – organy [ang. Organ]\n",
    "-   sax – saksofon [ang. Saxophone]\n",
    "-   tru – trąbka [ang. Trumpet]\n",
    "-   vio – skrzypce [ang. Violin]\n",
    "-   voi – wokale [ang. Voice]\n",
    " \n",
    "Dodatkowo, część z plików oferuje dodatkowe opisy dotyczące gatunku i występowanie bębnów:\n",
    "\n",
    "-   dru – występowanie bębnów\n",
    "-   nod – brak bębnów\n",
    "-   cou_fol – country-folk\n",
    "-   cla – klasyczna\n",
    "-   pop-roc – pop-rock\n",
    "-   lat-sou – latin-soul\n",
    "\n",
    "Nagrania to części muzyczne z aktualnych utworów z ubiegłego wieku. Co za tym idzie oferuje dużą różnorodność, a także jakość audio – utrudnia to zadanie klasyfikacji.\n",
    "W kwestii plików – baza składa się z 6705 nagrań audio składających się na 11 klas w różnej liczebności. Są to pliki 16 bit stereo .wav o częstotliwości próbkowania 44100 Hz.\n",
    "W początkowej fazie, ze względu na rozmiar bazy zawężono i wyrównano ilość plików .wav do 150 elementów (pierwszorzędnie usuwane były nagrania z oznaczeniami ‘dru’ i ‘nod’) na klasę, a co za tym idzie sumarycznie analizowana baza posiada 1650 elementów.\n",
    "\n",
    "Dokładna baza z wyselekcjonowanymi nagraniami znajduje się pod adresem:\n",
    "\n",
    "https://drive.google.com/drive/folders/1MegaYzPFYEbVGd5ica_SU_S_xoqSbcGF?usp=sharing\n",
    "\n",
    "Ten Jupyter Notebook jest poświęcony modelowi Lasu Losowego."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing\n",
    "\n",
    "Preprocessing zostanie przeprowadzony standardowo, zgodnie z tym co pojawiało się na zajęciach, a więc odpowiednie wczytanie i label'owanie nagrań, a także ustandaryzowanie danych za pomocą `Standard Scaler`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej import bibliotek :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries import :\n",
    "\n",
    "import scipy.stats\n",
    "import os\n",
    "import librosa\n",
    "import optuna\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, make_scorer, confusion_matrix, log_loss, precision_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przygotowanie dostępu do folderów z odpowiednimi klasami dla lepszej organizacji i listy do zautomatyzowania wczytywania nagrań :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of directories and lists :\n",
    "\n",
    "repoDir = Path.cwd()\n",
    "archiveDir = os.path.join(repoDir, \"IRMAS-TrainingData\")\n",
    "celDir = os.path.join(archiveDir, \"cel\")\n",
    "claDir = os.path.join(archiveDir, \"cla\")\n",
    "fluDir = os.path.join(archiveDir, \"flu\")\n",
    "gacDir = os.path.join(archiveDir, \"gac\")\n",
    "gelDir = os.path.join(archiveDir, \"gel\")\n",
    "orgDir = os.path.join(archiveDir, \"org\")\n",
    "piaDir = os.path.join(archiveDir, \"pia\")\n",
    "saxDir = os.path.join(archiveDir, \"sax\")\n",
    "truDir = os.path.join(archiveDir, \"tru\")\n",
    "vioDir = os.path.join(archiveDir, \"vio\")\n",
    "voiDir = os.path.join(archiveDir, \"voi\")\n",
    "\n",
    "\n",
    "listDir = [celDir, claDir, fluDir, gacDir, gelDir, orgDir, piaDir, saxDir, truDir, vioDir, voiDir]\n",
    "\n",
    "# Labels - zgodnie opisem w raporcie i wstępie\n",
    "# 0 - cel\n",
    "# 1 - cla\n",
    "# 2 - flu\n",
    "# 3 - gac\n",
    "# 4 - gel\n",
    "# 5 - org\n",
    "# 6 - pia\n",
    "# 7 - sax\n",
    "# 8 - tru\n",
    "# 9 - vio\n",
    "# 10 - voi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej przygotowanie danych do analizy. W formie `Dataframe'u` - dla wygody i ułatwienia posługiwania się danymi. Niżej check, czy wszystkie nagrania są w tej samej częstotliwości próbkowania :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_data</th>\n",
       "      <th>sampling_frequency</th>\n",
       "      <th>instrument_type</th>\n",
       "      <th>labels</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.022232056, 0.02468872, 0.02645874, 0.027236...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.0033721924, -0.003967285, -0.003479004, -0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00061035156, 0.0001373291, -0.0005950928, -...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.00024414062, -0.0013885498, -0.0026397705,...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.00018310547, 0.00076293945, 0.0011901855, ...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>[0.0059509277, 0.015350342, 0.023391724, 0.032...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>[-0.11528015, -0.15473938, -0.15000916, -0.113...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>[0.1661377, 0.15690613, 0.12338257, 0.07142639...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>[0.14439392, 0.12376404, 0.10206604, 0.0853881...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>[0.009460449, -0.008239746, -0.022598267, -0.0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            record_data  sampling_frequency  \\\n",
       "0     [0.022232056, 0.02468872, 0.02645874, 0.027236...               44100   \n",
       "1     [-0.0033721924, -0.003967285, -0.003479004, -0...               44100   \n",
       "2     [0.00061035156, 0.0001373291, -0.0005950928, -...               44100   \n",
       "3     [-0.00024414062, -0.0013885498, -0.0026397705,...               44100   \n",
       "4     [-0.00018310547, 0.00076293945, 0.0011901855, ...               44100   \n",
       "...                                                 ...                 ...   \n",
       "1645  [0.0059509277, 0.015350342, 0.023391724, 0.032...               44100   \n",
       "1646  [-0.11528015, -0.15473938, -0.15000916, -0.113...               44100   \n",
       "1647  [0.1661377, 0.15690613, 0.12338257, 0.07142639...               44100   \n",
       "1648  [0.14439392, 0.12376404, 0.10206604, 0.0853881...               44100   \n",
       "1649  [0.009460449, -0.008239746, -0.022598267, -0.0...               44100   \n",
       "\n",
       "     instrument_type  labels      genre  \n",
       "0                cel       0  classical  \n",
       "1                cel       0  classical  \n",
       "2                cel       0  classical  \n",
       "3                cel       0  classical  \n",
       "4                cel       0  classical  \n",
       "...              ...     ...        ...  \n",
       "1645             voi      10   pop_rock  \n",
       "1646             voi      10   pop_rock  \n",
       "1647             voi      10   pop_rock  \n",
       "1648             voi      10   pop_rock  \n",
       "1649             voi      10   pop_rock  \n",
       "\n",
       "[1650 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creation of database :\n",
    "\n",
    "records = []\n",
    "wavNames = []\n",
    "labels = []\n",
    "genre = []\n",
    "for iter, list in enumerate(listDir):\n",
    "    os.chdir(repoDir)    \n",
    "    currentList = os.listdir(list)\n",
    "    os.chdir(list)\n",
    "    for recording in currentList:\n",
    "        records.append(librosa.load(recording, sr=44100))\n",
    "        wavNames.append(str(recording[1:4]))\n",
    "        labels.append(iter)\n",
    "        if \"[cla]\" in str(recording):\n",
    "            genre.append(\"classical\")\n",
    "        elif \"[jaz_blu]\" in str(recording):\n",
    "            genre.append(\"jazz_blues\")\n",
    "        elif \"[pop_roc]\" in str(recording):\n",
    "            genre.append(\"pop_rock\")\n",
    "        elif \"[cou_fol]\" in str(recording):\n",
    "            genre.append(\"country_folk\")\n",
    "        elif \"[lat_sou]\" in str(recording):\n",
    "            genre.append(\"latin_soul\")\n",
    "\n",
    "database = pandas.DataFrame(data=records)\n",
    "database.columns = [\"record_data\", \"sampling_frequency\"]\n",
    "database[\"instrument_type\"] = wavNames\n",
    "database[\"labels\"] = labels\n",
    "database[\"genre\"] = genre\n",
    "display(database)\n",
    "os.chdir(repoDir)\n",
    "\n",
    "# Sampling frequency check :\n",
    "\n",
    "for frequency in database[\"sampling_frequency\"]:\n",
    "    if frequency == 44100:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Freqency doesn't match 44100 Hz.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolejno trzeba wyekstrachować cechy. Użyte zostanie 13 cech MFCC, a dodatkowo wykorzystane zostanie `MFCC_delta` oraz `MFCC_delta_delta`. Wszystkie dane zostaną przedstawione w `DataFrame'ie` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_data</th>\n",
       "      <th>sampling_frequency</th>\n",
       "      <th>instrument_type</th>\n",
       "      <th>labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>mfcc_delta</th>\n",
       "      <th>mfcc_delta_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.022232056, 0.02468872, 0.02645874, 0.027236...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-388.13446, -423.61435, -473.78613, -472.044...</td>\n",
       "      <td>[[-8.185071, -8.185071, -8.185071, -8.185071, ...</td>\n",
       "      <td>[[5.1802053, 5.1802053, 5.1802053, 5.1802053, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.0033721924, -0.003967285, -0.003479004, -0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-380.3471, -347.56705, -341.9458, -340.2455,...</td>\n",
       "      <td>[[3.440472, 3.440472, 3.440472, 3.440472, 3.44...</td>\n",
       "      <td>[[-2.7836533, -2.7836533, -2.7836533, -2.78365...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00061035156, 0.0001373291, -0.0005950928, -...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-444.47504, -436.25427, -442.07135, -440.382...</td>\n",
       "      <td>[[1.7043172, 1.7043172, 1.7043172, 1.7043172, ...</td>\n",
       "      <td>[[0.64747626, 0.64747626, 0.64747626, 0.647476...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.00024414062, -0.0013885498, -0.0026397705,...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-499.21375, -478.78656, -483.94357, -485.582...</td>\n",
       "      <td>[[1.6173273, 1.6173273, 1.6173273, 1.6173273, ...</td>\n",
       "      <td>[[-0.55970395, -0.55970395, -0.55970395, -0.55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.00018310547, 0.00076293945, 0.0011901855, ...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-491.483, -463.85297, -464.4726, -464.46667,...</td>\n",
       "      <td>[[1.5724477, 1.5724477, 1.5724477, 1.5724477, ...</td>\n",
       "      <td>[[-2.0295393, -2.0295393, -2.0295393, -2.02953...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>[0.0059509277, 0.015350342, 0.023391724, 0.032...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-209.98701, -174.61427, -171.96695, -177.942...</td>\n",
       "      <td>[[-1.7447826, -1.7447826, -1.7447826, -1.74478...</td>\n",
       "      <td>[[-3.0017266, -3.0017266, -3.0017266, -3.00172...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>[-0.11528015, -0.15473938, -0.15000916, -0.113...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-142.983, -119.2241, -121.361145, -122.55564...</td>\n",
       "      <td>[[-0.27515614, -0.27515614, -0.27515614, -0.27...</td>\n",
       "      <td>[[-1.6264538, -1.6264538, -1.6264538, -1.62645...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>[0.1661377, 0.15690613, 0.12338257, 0.07142639...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-127.09217, -121.634254, -144.1307, -150.183...</td>\n",
       "      <td>[[10.00528, 10.00528, 10.00528, 10.00528, 10.0...</td>\n",
       "      <td>[[7.137136, 7.137136, 7.137136, 7.137136, 7.13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>[0.14439392, 0.12376404, 0.10206604, 0.0853881...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-125.003006, -52.63265, -53.847103, -72.9391...</td>\n",
       "      <td>[[-2.2320504, -2.2320504, -2.2320504, -2.23205...</td>\n",
       "      <td>[[-3.3040605, -3.3040605, -3.3040605, -3.30406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>[0.009460449, -0.008239746, -0.022598267, -0.0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-159.35313, -133.85146, -129.30109, -133.604...</td>\n",
       "      <td>[[1.0767472, 1.0767472, 1.0767472, 1.0767472, ...</td>\n",
       "      <td>[[-1.2744101, -1.2744101, -1.2744101, -1.27441...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            record_data  sampling_frequency  \\\n",
       "0     [0.022232056, 0.02468872, 0.02645874, 0.027236...               44100   \n",
       "1     [-0.0033721924, -0.003967285, -0.003479004, -0...               44100   \n",
       "2     [0.00061035156, 0.0001373291, -0.0005950928, -...               44100   \n",
       "3     [-0.00024414062, -0.0013885498, -0.0026397705,...               44100   \n",
       "4     [-0.00018310547, 0.00076293945, 0.0011901855, ...               44100   \n",
       "...                                                 ...                 ...   \n",
       "1645  [0.0059509277, 0.015350342, 0.023391724, 0.032...               44100   \n",
       "1646  [-0.11528015, -0.15473938, -0.15000916, -0.113...               44100   \n",
       "1647  [0.1661377, 0.15690613, 0.12338257, 0.07142639...               44100   \n",
       "1648  [0.14439392, 0.12376404, 0.10206604, 0.0853881...               44100   \n",
       "1649  [0.009460449, -0.008239746, -0.022598267, -0.0...               44100   \n",
       "\n",
       "     instrument_type  labels      genre  \\\n",
       "0                cel       0  classical   \n",
       "1                cel       0  classical   \n",
       "2                cel       0  classical   \n",
       "3                cel       0  classical   \n",
       "4                cel       0  classical   \n",
       "...              ...     ...        ...   \n",
       "1645             voi      10   pop_rock   \n",
       "1646             voi      10   pop_rock   \n",
       "1647             voi      10   pop_rock   \n",
       "1648             voi      10   pop_rock   \n",
       "1649             voi      10   pop_rock   \n",
       "\n",
       "                                                   mfcc  \\\n",
       "0     [[-388.13446, -423.61435, -473.78613, -472.044...   \n",
       "1     [[-380.3471, -347.56705, -341.9458, -340.2455,...   \n",
       "2     [[-444.47504, -436.25427, -442.07135, -440.382...   \n",
       "3     [[-499.21375, -478.78656, -483.94357, -485.582...   \n",
       "4     [[-491.483, -463.85297, -464.4726, -464.46667,...   \n",
       "...                                                 ...   \n",
       "1645  [[-209.98701, -174.61427, -171.96695, -177.942...   \n",
       "1646  [[-142.983, -119.2241, -121.361145, -122.55564...   \n",
       "1647  [[-127.09217, -121.634254, -144.1307, -150.183...   \n",
       "1648  [[-125.003006, -52.63265, -53.847103, -72.9391...   \n",
       "1649  [[-159.35313, -133.85146, -129.30109, -133.604...   \n",
       "\n",
       "                                             mfcc_delta  \\\n",
       "0     [[-8.185071, -8.185071, -8.185071, -8.185071, ...   \n",
       "1     [[3.440472, 3.440472, 3.440472, 3.440472, 3.44...   \n",
       "2     [[1.7043172, 1.7043172, 1.7043172, 1.7043172, ...   \n",
       "3     [[1.6173273, 1.6173273, 1.6173273, 1.6173273, ...   \n",
       "4     [[1.5724477, 1.5724477, 1.5724477, 1.5724477, ...   \n",
       "...                                                 ...   \n",
       "1645  [[-1.7447826, -1.7447826, -1.7447826, -1.74478...   \n",
       "1646  [[-0.27515614, -0.27515614, -0.27515614, -0.27...   \n",
       "1647  [[10.00528, 10.00528, 10.00528, 10.00528, 10.0...   \n",
       "1648  [[-2.2320504, -2.2320504, -2.2320504, -2.23205...   \n",
       "1649  [[1.0767472, 1.0767472, 1.0767472, 1.0767472, ...   \n",
       "\n",
       "                                       mfcc_delta_delta  \n",
       "0     [[5.1802053, 5.1802053, 5.1802053, 5.1802053, ...  \n",
       "1     [[-2.7836533, -2.7836533, -2.7836533, -2.78365...  \n",
       "2     [[0.64747626, 0.64747626, 0.64747626, 0.647476...  \n",
       "3     [[-0.55970395, -0.55970395, -0.55970395, -0.55...  \n",
       "4     [[-2.0295393, -2.0295393, -2.0295393, -2.02953...  \n",
       "...                                                 ...  \n",
       "1645  [[-3.0017266, -3.0017266, -3.0017266, -3.00172...  \n",
       "1646  [[-1.6264538, -1.6264538, -1.6264538, -1.62645...  \n",
       "1647  [[7.137136, 7.137136, 7.137136, 7.137136, 7.13...  \n",
       "1648  [[-3.3040605, -3.3040605, -3.3040605, -3.30406...  \n",
       "1649  [[-1.2744101, -1.2744101, -1.2744101, -1.27441...  \n",
       "\n",
       "[1650 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extracting MFCC coefficents\n",
    "\n",
    "mfccs = []\n",
    "mfccs_delta = []\n",
    "mfccs_deltasq = []\n",
    "for data in database[\"record_data\"]:\n",
    "    mfcc = librosa.feature.mfcc(y=data, sr=22050, n_mfcc=13)\n",
    "    mfccs.append(mfcc)\n",
    "    mfccs_delta.append(librosa.feature.delta(mfcc))\n",
    "    mfccs_deltasq.append(librosa.feature.delta(mfcc, order=2))\n",
    "\n",
    "database[\"mfcc\"] = mfccs\n",
    "database[\"mfcc_delta\"] = mfccs_delta\n",
    "database[\"mfcc_delta_delta\"] = mfccs_deltasq\n",
    "\n",
    "display(database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście, ciężko będzie użyć `MFCC`, które jest niezgodne co do długości z innymi. Jednak nasze nagrania mają z góry przygotowane nagrania tak, że długości wszystkich `MFCC` wynoszą : 3367 elementów. Oczywiście, jeżeli zachowamy wariant z 13 cechami `MFCC`. Przy zmianie powinno być rówinież w porządku, jednak liczba będzie większa lub mniejsza i należy to zweryfikować. Jednak trzeba to sprawdzić kodem poniżej :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_len = 3367\n",
    "\n",
    "for item in database[\"mfcc\"]:\n",
    "    if np.size(item) != mfcc_len:\n",
    "        print(\"Lenght of arrays doesn't match !\")\n",
    "\n",
    "for item in database[\"mfcc_delta\"]:\n",
    "    if np.size(item) != mfcc_len:\n",
    "        print(\"Lenght of arrays doesn't match !\")\n",
    "\n",
    "for item in database[\"mfcc_delta_delta\"]:\n",
    "    if np.size(item) != mfcc_len:\n",
    "        print(\"Lenght of arrays doesn't match !\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy wykorzystać również podejście związane z 'parametrami' `MFCC`. Damy radę przeanalizować :\n",
    "\n",
    "- wartość średnią, \n",
    "- odchylenie standardowe, \n",
    "- medianę, \n",
    "- I i III kwartyl, \n",
    "- rozrzut pomiędzy 10 i 90 percentylem, \n",
    "- kurtozę, \n",
    "- skośność, \n",
    "- wartość minimalną,\n",
    "- wartość maksymalną.\n",
    "\n",
    "Otrzymamy tutaj 10 parametrów na każdą ramkę `MFCC`, czyli 130 parametrów na każdy sygnał i tyle będzie opisywać dany element. Łącznie wszystkich otrzymamy 19200 i wpiszemy kolejno do `DataFrame'u`. Ze względu na taką budowę naszej bazy danych, musimy przeiterować, wyliczenie kolejnych wartości po kolejnych ramkach `MFCC` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_data</th>\n",
       "      <th>sampling_frequency</th>\n",
       "      <th>instrument_type</th>\n",
       "      <th>labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>mfcc_delta</th>\n",
       "      <th>mfcc_delta_delta</th>\n",
       "      <th>mfcc_parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.022232056, 0.02468872, 0.02645874, 0.027236...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-388.13446, -423.61435, -473.78613, -472.044...</td>\n",
       "      <td>[[-8.185071, -8.185071, -8.185071, -8.185071, ...</td>\n",
       "      <td>[[5.1802053, 5.1802053, 5.1802053, 5.1802053, ...</td>\n",
       "      <td>[-453.2663269042969, 22.335796356201172, -451....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.0033721924, -0.003967285, -0.003479004, -0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-380.3471, -347.56705, -341.9458, -340.2455,...</td>\n",
       "      <td>[[3.440472, 3.440472, 3.440472, 3.440472, 3.44...</td>\n",
       "      <td>[[-2.7836533, -2.7836533, -2.7836533, -2.78365...</td>\n",
       "      <td>[-383.6215515136719, 25.697067260742188, -380....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00061035156, 0.0001373291, -0.0005950928, -...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-444.47504, -436.25427, -442.07135, -440.382...</td>\n",
       "      <td>[[1.7043172, 1.7043172, 1.7043172, 1.7043172, ...</td>\n",
       "      <td>[[0.64747626, 0.64747626, 0.64747626, 0.647476...</td>\n",
       "      <td>[-472.3026428222656, 20.734647750854492, -470....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.00024414062, -0.0013885498, -0.0026397705,...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-499.21375, -478.78656, -483.94357, -485.582...</td>\n",
       "      <td>[[1.6173273, 1.6173273, 1.6173273, 1.6173273, ...</td>\n",
       "      <td>[[-0.55970395, -0.55970395, -0.55970395, -0.55...</td>\n",
       "      <td>[-470.0331726074219, 17.2320613861084, -471.61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.00018310547, 0.00076293945, 0.0011901855, ...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[[-491.483, -463.85297, -464.4726, -464.46667,...</td>\n",
       "      <td>[[1.5724477, 1.5724477, 1.5724477, 1.5724477, ...</td>\n",
       "      <td>[[-2.0295393, -2.0295393, -2.0295393, -2.02953...</td>\n",
       "      <td>[-452.78167724609375, 26.359832763671875, -461...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>[0.0059509277, 0.015350342, 0.023391724, 0.032...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-209.98701, -174.61427, -171.96695, -177.942...</td>\n",
       "      <td>[[-1.7447826, -1.7447826, -1.7447826, -1.74478...</td>\n",
       "      <td>[[-3.0017266, -3.0017266, -3.0017266, -3.00172...</td>\n",
       "      <td>[-202.75323486328125, 25.102352142333984, -202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>[-0.11528015, -0.15473938, -0.15000916, -0.113...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-142.983, -119.2241, -121.361145, -122.55564...</td>\n",
       "      <td>[[-0.27515614, -0.27515614, -0.27515614, -0.27...</td>\n",
       "      <td>[[-1.6264538, -1.6264538, -1.6264538, -1.62645...</td>\n",
       "      <td>[-153.88998413085938, 37.77930450439453, -157....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>[0.1661377, 0.15690613, 0.12338257, 0.07142639...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-127.09217, -121.634254, -144.1307, -150.183...</td>\n",
       "      <td>[[10.00528, 10.00528, 10.00528, 10.00528, 10.0...</td>\n",
       "      <td>[[7.137136, 7.137136, 7.137136, 7.137136, 7.13...</td>\n",
       "      <td>[-110.45230865478516, 27.135652542114258, -114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>[0.14439392, 0.12376404, 0.10206604, 0.0853881...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-125.003006, -52.63265, -53.847103, -72.9391...</td>\n",
       "      <td>[[-2.2320504, -2.2320504, -2.2320504, -2.23205...</td>\n",
       "      <td>[[-3.3040605, -3.3040605, -3.3040605, -3.30406...</td>\n",
       "      <td>[-105.6734390258789, 29.895414352416992, -106....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>[0.009460449, -0.008239746, -0.022598267, -0.0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[[-159.35313, -133.85146, -129.30109, -133.604...</td>\n",
       "      <td>[[1.0767472, 1.0767472, 1.0767472, 1.0767472, ...</td>\n",
       "      <td>[[-1.2744101, -1.2744101, -1.2744101, -1.27441...</td>\n",
       "      <td>[-144.6693878173828, 24.62249183654785, -149.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            record_data  sampling_frequency  \\\n",
       "0     [0.022232056, 0.02468872, 0.02645874, 0.027236...               44100   \n",
       "1     [-0.0033721924, -0.003967285, -0.003479004, -0...               44100   \n",
       "2     [0.00061035156, 0.0001373291, -0.0005950928, -...               44100   \n",
       "3     [-0.00024414062, -0.0013885498, -0.0026397705,...               44100   \n",
       "4     [-0.00018310547, 0.00076293945, 0.0011901855, ...               44100   \n",
       "...                                                 ...                 ...   \n",
       "1645  [0.0059509277, 0.015350342, 0.023391724, 0.032...               44100   \n",
       "1646  [-0.11528015, -0.15473938, -0.15000916, -0.113...               44100   \n",
       "1647  [0.1661377, 0.15690613, 0.12338257, 0.07142639...               44100   \n",
       "1648  [0.14439392, 0.12376404, 0.10206604, 0.0853881...               44100   \n",
       "1649  [0.009460449, -0.008239746, -0.022598267, -0.0...               44100   \n",
       "\n",
       "     instrument_type  labels      genre  \\\n",
       "0                cel       0  classical   \n",
       "1                cel       0  classical   \n",
       "2                cel       0  classical   \n",
       "3                cel       0  classical   \n",
       "4                cel       0  classical   \n",
       "...              ...     ...        ...   \n",
       "1645             voi      10   pop_rock   \n",
       "1646             voi      10   pop_rock   \n",
       "1647             voi      10   pop_rock   \n",
       "1648             voi      10   pop_rock   \n",
       "1649             voi      10   pop_rock   \n",
       "\n",
       "                                                   mfcc  \\\n",
       "0     [[-388.13446, -423.61435, -473.78613, -472.044...   \n",
       "1     [[-380.3471, -347.56705, -341.9458, -340.2455,...   \n",
       "2     [[-444.47504, -436.25427, -442.07135, -440.382...   \n",
       "3     [[-499.21375, -478.78656, -483.94357, -485.582...   \n",
       "4     [[-491.483, -463.85297, -464.4726, -464.46667,...   \n",
       "...                                                 ...   \n",
       "1645  [[-209.98701, -174.61427, -171.96695, -177.942...   \n",
       "1646  [[-142.983, -119.2241, -121.361145, -122.55564...   \n",
       "1647  [[-127.09217, -121.634254, -144.1307, -150.183...   \n",
       "1648  [[-125.003006, -52.63265, -53.847103, -72.9391...   \n",
       "1649  [[-159.35313, -133.85146, -129.30109, -133.604...   \n",
       "\n",
       "                                             mfcc_delta  \\\n",
       "0     [[-8.185071, -8.185071, -8.185071, -8.185071, ...   \n",
       "1     [[3.440472, 3.440472, 3.440472, 3.440472, 3.44...   \n",
       "2     [[1.7043172, 1.7043172, 1.7043172, 1.7043172, ...   \n",
       "3     [[1.6173273, 1.6173273, 1.6173273, 1.6173273, ...   \n",
       "4     [[1.5724477, 1.5724477, 1.5724477, 1.5724477, ...   \n",
       "...                                                 ...   \n",
       "1645  [[-1.7447826, -1.7447826, -1.7447826, -1.74478...   \n",
       "1646  [[-0.27515614, -0.27515614, -0.27515614, -0.27...   \n",
       "1647  [[10.00528, 10.00528, 10.00528, 10.00528, 10.0...   \n",
       "1648  [[-2.2320504, -2.2320504, -2.2320504, -2.23205...   \n",
       "1649  [[1.0767472, 1.0767472, 1.0767472, 1.0767472, ...   \n",
       "\n",
       "                                       mfcc_delta_delta  \\\n",
       "0     [[5.1802053, 5.1802053, 5.1802053, 5.1802053, ...   \n",
       "1     [[-2.7836533, -2.7836533, -2.7836533, -2.78365...   \n",
       "2     [[0.64747626, 0.64747626, 0.64747626, 0.647476...   \n",
       "3     [[-0.55970395, -0.55970395, -0.55970395, -0.55...   \n",
       "4     [[-2.0295393, -2.0295393, -2.0295393, -2.02953...   \n",
       "...                                                 ...   \n",
       "1645  [[-3.0017266, -3.0017266, -3.0017266, -3.00172...   \n",
       "1646  [[-1.6264538, -1.6264538, -1.6264538, -1.62645...   \n",
       "1647  [[7.137136, 7.137136, 7.137136, 7.137136, 7.13...   \n",
       "1648  [[-3.3040605, -3.3040605, -3.3040605, -3.30406...   \n",
       "1649  [[-1.2744101, -1.2744101, -1.2744101, -1.27441...   \n",
       "\n",
       "                                        mfcc_parameters  \n",
       "0     [-453.2663269042969, 22.335796356201172, -451....  \n",
       "1     [-383.6215515136719, 25.697067260742188, -380....  \n",
       "2     [-472.3026428222656, 20.734647750854492, -470....  \n",
       "3     [-470.0331726074219, 17.2320613861084, -471.61...  \n",
       "4     [-452.78167724609375, 26.359832763671875, -461...  \n",
       "...                                                 ...  \n",
       "1645  [-202.75323486328125, 25.102352142333984, -202...  \n",
       "1646  [-153.88998413085938, 37.77930450439453, -157....  \n",
       "1647  [-110.45230865478516, 27.135652542114258, -114...  \n",
       "1648  [-105.6734390258789, 29.895414352416992, -106....  \n",
       "1649  [-144.6693878173828, 24.62249183654785, -149.4...  \n",
       "\n",
       "[1650 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mfcc_parameters = []\n",
    "for iteration, value in enumerate(database[\"mfcc\"]):\n",
    "    mfcc_stack = []\n",
    "    for i in range(0,12):\n",
    "        data_stack = np.hstack((np.mean(database[\"mfcc\"][iteration][i]), \n",
    "                    np.std(database[\"mfcc\"][iteration][i]), \n",
    "                    np.median(database[\"mfcc\"][iteration][i]), \n",
    "                    np.percentile(database[\"mfcc\"][iteration][i], 25), \n",
    "                    np.percentile(database[\"mfcc\"][iteration][i], 75), \n",
    "                    scipy.stats.iqr(database[\"mfcc\"][iteration][i], rng=(10, 90)),\n",
    "                    scipy.stats.kurtosis(database[\"mfcc\"][iteration][i]),\n",
    "                    scipy.stats.skew(database[\"mfcc\"][iteration][i]),\n",
    "                    np.min(database[\"mfcc\"][iteration][i]),\n",
    "                    np.max(database[\"mfcc\"][iteration][i])\n",
    "                    ))\n",
    "        mfcc_stack = np.hstack((mfcc_stack, data_stack))\n",
    "    mfcc_parameters.append(mfcc_stack)\n",
    "\n",
    "database[\"mfcc_parameters\"] = mfcc_parameters\n",
    "display(database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możemy, więc sprawidzić też, które z podejść (`MFCC`, `MFCC-delta`, `MFCC-delta-delta` czy parametry) dadzą najlepsze wyniki. Jednak w przypadku pierwszych trzech można pomyśleć o redukcji wymiarowości, gdyż jedno `MFCC` to, aż 3367 elementów. Dodatkowo w przypadku `MFCC` i delt musimy pamiętać o spłaszczeniu danych, by móc odpowiednio przeprocesować je przez `train_test_split` oraz `StandardScaler`. Zostanie to wykonane poniżej i podmienione zostaną kolumny z wartościami, na te ze spłaszczonymi `MFCC`. Jest to wykonywane pod koniec przez wzgląd na poprawne obliczenie wcześniejszych 'parametrów'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_data</th>\n",
       "      <th>sampling_frequency</th>\n",
       "      <th>instrument_type</th>\n",
       "      <th>labels</th>\n",
       "      <th>genre</th>\n",
       "      <th>mfcc_parameters</th>\n",
       "      <th>mfcc</th>\n",
       "      <th>mfcc_delta</th>\n",
       "      <th>mfcc_deltasq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.022232056, 0.02468872, 0.02645874, 0.027236...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[-453.2663269042969, 22.335796356201172, -451....</td>\n",
       "      <td>[-388.13446, -423.61435, -473.78613, -472.0447...</td>\n",
       "      <td>[-8.185071, -8.185071, -8.185071, -8.185071, -...</td>\n",
       "      <td>[5.1802053, 5.1802053, 5.1802053, 5.1802053, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.0033721924, -0.003967285, -0.003479004, -0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[-383.6215515136719, 25.697067260742188, -380....</td>\n",
       "      <td>[-380.3471, -347.56705, -341.9458, -340.2455, ...</td>\n",
       "      <td>[3.440472, 3.440472, 3.440472, 3.440472, 3.440...</td>\n",
       "      <td>[-2.7836533, -2.7836533, -2.7836533, -2.783653...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0.00061035156, 0.0001373291, -0.0005950928, -...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[-472.3026428222656, 20.734647750854492, -470....</td>\n",
       "      <td>[-444.47504, -436.25427, -442.07135, -440.3829...</td>\n",
       "      <td>[1.7043172, 1.7043172, 1.7043172, 1.7043172, 1...</td>\n",
       "      <td>[0.64747626, 0.64747626, 0.64747626, 0.6474762...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.00024414062, -0.0013885498, -0.0026397705,...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[-470.0331726074219, 17.2320613861084, -471.61...</td>\n",
       "      <td>[-499.21375, -478.78656, -483.94357, -485.5826...</td>\n",
       "      <td>[1.6173273, 1.6173273, 1.6173273, 1.6173273, 1...</td>\n",
       "      <td>[-0.55970395, -0.55970395, -0.55970395, -0.559...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.00018310547, 0.00076293945, 0.0011901855, ...</td>\n",
       "      <td>44100</td>\n",
       "      <td>cel</td>\n",
       "      <td>0</td>\n",
       "      <td>classical</td>\n",
       "      <td>[-452.78167724609375, 26.359832763671875, -461...</td>\n",
       "      <td>[-491.483, -463.85297, -464.4726, -464.46667, ...</td>\n",
       "      <td>[1.5724477, 1.5724477, 1.5724477, 1.5724477, 1...</td>\n",
       "      <td>[-2.0295393, -2.0295393, -2.0295393, -2.029539...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>[0.0059509277, 0.015350342, 0.023391724, 0.032...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[-202.75323486328125, 25.102352142333984, -202...</td>\n",
       "      <td>[-209.98701, -174.61427, -171.96695, -177.9427...</td>\n",
       "      <td>[-1.7447826, -1.7447826, -1.7447826, -1.744782...</td>\n",
       "      <td>[-3.0017266, -3.0017266, -3.0017266, -3.001726...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>[-0.11528015, -0.15473938, -0.15000916, -0.113...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[-153.88998413085938, 37.77930450439453, -157....</td>\n",
       "      <td>[-142.983, -119.2241, -121.361145, -122.55564,...</td>\n",
       "      <td>[-0.27515614, -0.27515614, -0.27515614, -0.275...</td>\n",
       "      <td>[-1.6264538, -1.6264538, -1.6264538, -1.626453...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>[0.1661377, 0.15690613, 0.12338257, 0.07142639...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[-110.45230865478516, 27.135652542114258, -114...</td>\n",
       "      <td>[-127.09217, -121.634254, -144.1307, -150.1832...</td>\n",
       "      <td>[10.00528, 10.00528, 10.00528, 10.00528, 10.00...</td>\n",
       "      <td>[7.137136, 7.137136, 7.137136, 7.137136, 7.137...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>[0.14439392, 0.12376404, 0.10206604, 0.0853881...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[-105.6734390258789, 29.895414352416992, -106....</td>\n",
       "      <td>[-125.003006, -52.63265, -53.847103, -72.93917...</td>\n",
       "      <td>[-2.2320504, -2.2320504, -2.2320504, -2.232050...</td>\n",
       "      <td>[-3.3040605, -3.3040605, -3.3040605, -3.304060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>[0.009460449, -0.008239746, -0.022598267, -0.0...</td>\n",
       "      <td>44100</td>\n",
       "      <td>voi</td>\n",
       "      <td>10</td>\n",
       "      <td>pop_rock</td>\n",
       "      <td>[-144.6693878173828, 24.62249183654785, -149.4...</td>\n",
       "      <td>[-159.35313, -133.85146, -129.30109, -133.6044...</td>\n",
       "      <td>[1.0767472, 1.0767472, 1.0767472, 1.0767472, 1...</td>\n",
       "      <td>[-1.2744101, -1.2744101, -1.2744101, -1.274410...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            record_data  sampling_frequency  \\\n",
       "0     [0.022232056, 0.02468872, 0.02645874, 0.027236...               44100   \n",
       "1     [-0.0033721924, -0.003967285, -0.003479004, -0...               44100   \n",
       "2     [0.00061035156, 0.0001373291, -0.0005950928, -...               44100   \n",
       "3     [-0.00024414062, -0.0013885498, -0.0026397705,...               44100   \n",
       "4     [-0.00018310547, 0.00076293945, 0.0011901855, ...               44100   \n",
       "...                                                 ...                 ...   \n",
       "1645  [0.0059509277, 0.015350342, 0.023391724, 0.032...               44100   \n",
       "1646  [-0.11528015, -0.15473938, -0.15000916, -0.113...               44100   \n",
       "1647  [0.1661377, 0.15690613, 0.12338257, 0.07142639...               44100   \n",
       "1648  [0.14439392, 0.12376404, 0.10206604, 0.0853881...               44100   \n",
       "1649  [0.009460449, -0.008239746, -0.022598267, -0.0...               44100   \n",
       "\n",
       "     instrument_type  labels      genre  \\\n",
       "0                cel       0  classical   \n",
       "1                cel       0  classical   \n",
       "2                cel       0  classical   \n",
       "3                cel       0  classical   \n",
       "4                cel       0  classical   \n",
       "...              ...     ...        ...   \n",
       "1645             voi      10   pop_rock   \n",
       "1646             voi      10   pop_rock   \n",
       "1647             voi      10   pop_rock   \n",
       "1648             voi      10   pop_rock   \n",
       "1649             voi      10   pop_rock   \n",
       "\n",
       "                                        mfcc_parameters  \\\n",
       "0     [-453.2663269042969, 22.335796356201172, -451....   \n",
       "1     [-383.6215515136719, 25.697067260742188, -380....   \n",
       "2     [-472.3026428222656, 20.734647750854492, -470....   \n",
       "3     [-470.0331726074219, 17.2320613861084, -471.61...   \n",
       "4     [-452.78167724609375, 26.359832763671875, -461...   \n",
       "...                                                 ...   \n",
       "1645  [-202.75323486328125, 25.102352142333984, -202...   \n",
       "1646  [-153.88998413085938, 37.77930450439453, -157....   \n",
       "1647  [-110.45230865478516, 27.135652542114258, -114...   \n",
       "1648  [-105.6734390258789, 29.895414352416992, -106....   \n",
       "1649  [-144.6693878173828, 24.62249183654785, -149.4...   \n",
       "\n",
       "                                                   mfcc  \\\n",
       "0     [-388.13446, -423.61435, -473.78613, -472.0447...   \n",
       "1     [-380.3471, -347.56705, -341.9458, -340.2455, ...   \n",
       "2     [-444.47504, -436.25427, -442.07135, -440.3829...   \n",
       "3     [-499.21375, -478.78656, -483.94357, -485.5826...   \n",
       "4     [-491.483, -463.85297, -464.4726, -464.46667, ...   \n",
       "...                                                 ...   \n",
       "1645  [-209.98701, -174.61427, -171.96695, -177.9427...   \n",
       "1646  [-142.983, -119.2241, -121.361145, -122.55564,...   \n",
       "1647  [-127.09217, -121.634254, -144.1307, -150.1832...   \n",
       "1648  [-125.003006, -52.63265, -53.847103, -72.93917...   \n",
       "1649  [-159.35313, -133.85146, -129.30109, -133.6044...   \n",
       "\n",
       "                                             mfcc_delta  \\\n",
       "0     [-8.185071, -8.185071, -8.185071, -8.185071, -...   \n",
       "1     [3.440472, 3.440472, 3.440472, 3.440472, 3.440...   \n",
       "2     [1.7043172, 1.7043172, 1.7043172, 1.7043172, 1...   \n",
       "3     [1.6173273, 1.6173273, 1.6173273, 1.6173273, 1...   \n",
       "4     [1.5724477, 1.5724477, 1.5724477, 1.5724477, 1...   \n",
       "...                                                 ...   \n",
       "1645  [-1.7447826, -1.7447826, -1.7447826, -1.744782...   \n",
       "1646  [-0.27515614, -0.27515614, -0.27515614, -0.275...   \n",
       "1647  [10.00528, 10.00528, 10.00528, 10.00528, 10.00...   \n",
       "1648  [-2.2320504, -2.2320504, -2.2320504, -2.232050...   \n",
       "1649  [1.0767472, 1.0767472, 1.0767472, 1.0767472, 1...   \n",
       "\n",
       "                                           mfcc_deltasq  \n",
       "0     [5.1802053, 5.1802053, 5.1802053, 5.1802053, 5...  \n",
       "1     [-2.7836533, -2.7836533, -2.7836533, -2.783653...  \n",
       "2     [0.64747626, 0.64747626, 0.64747626, 0.6474762...  \n",
       "3     [-0.55970395, -0.55970395, -0.55970395, -0.559...  \n",
       "4     [-2.0295393, -2.0295393, -2.0295393, -2.029539...  \n",
       "...                                                 ...  \n",
       "1645  [-3.0017266, -3.0017266, -3.0017266, -3.001726...  \n",
       "1646  [-1.6264538, -1.6264538, -1.6264538, -1.626453...  \n",
       "1647  [7.137136, 7.137136, 7.137136, 7.137136, 7.137...  \n",
       "1648  [-3.3040605, -3.3040605, -3.3040605, -3.304060...  \n",
       "1649  [-1.2744101, -1.2744101, -1.2744101, -1.274410...  \n",
       "\n",
       "[1650 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mfcc_flatten = []\n",
    "mfcc_flatten_delta = []\n",
    "mfcc_flatten_delta_delta = []\n",
    "for i in range (0,1650):\n",
    "    mfcc_flatten.append(database['mfcc'][i].flatten())\n",
    "    mfcc_flatten_delta.append(database['mfcc_delta'][i].flatten())\n",
    "    mfcc_flatten_delta_delta.append(database['mfcc_delta_delta'][i].flatten())\n",
    "\n",
    "database.drop('mfcc', axis=1, inplace=True)\n",
    "database.drop('mfcc_delta', axis=1, inplace=True)\n",
    "database.drop('mfcc_delta_delta', axis=1, inplace=True)\n",
    "database['mfcc'] = mfcc_flatten\n",
    "database['mfcc_delta'] = mfcc_flatten_delta\n",
    "database['mfcc_deltasq'] = mfcc_flatten_delta_delta\n",
    "display(database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Przygotowanie zbiorów uczących i modelu.\n",
    "\n",
    "Teraz można zająć się przygotowaniem danych i modelu. Użyty zostanie `train_test_split` (nie będzie stosowana crosswalidacja, ze względu na małą liczność klas). By współpracować z `sklearn` i `pandasem`, dane wyciągnięte z `DataFrame'u` konwertowane będą na listy array'ów, by można podzielone zbiory odpowiednio jeszcze ustandaryzować `StandardScaler'em`. W przypadku MFCC i delt musimy również pamiętać o spłaszczeniu danych - jednak w poniższym podejściu zastosowane będą 'parametry' MFCC, a zostały przygotowane już w odpowiedniej formie :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(database[\"mfcc_parameters\"].to_list(), database[\"labels\"].to_list(), test_size=0.2, random_state=42, stratify=database[\"labels\"].to_list())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poniższy kod i czynności posiadają jedynie niezbędny opis, głównie nawiązujący do przeprowadzonych działań. Część analizy i wniosków jest przedstawiona w raporcie w pliku .pdf."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizowanym modelem jest model `ExtraTreesClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na początek można sprawdzić jak poradzi sobie podstawowy model, bez żadnych modyfikacji i przygotowania :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train)\n",
    "preds_test = model.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczone metryki znajdują się poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[21  1  3  3  1  1  0  0  0  0  0]\n",
      " [ 3 16  2  4  0  0  0  0  1  1  3]\n",
      " [ 1  0 19  1  0  3  2  0  0  4  0]\n",
      " [ 1  3  1 21  0  0  4  0  0  0  0]\n",
      " [ 0  1  0  3 17  2  0  0  2  1  4]\n",
      " [ 0  0  0  1  1 24  1  0  0  0  3]\n",
      " [ 0  1  0  1  1  0 19  1  2  2  3]\n",
      " [ 5  1  0  1  0  0  2 15  4  1  1]\n",
      " [ 0  2  2  0  0  0  1  1 21  0  3]\n",
      " [ 2  0  3  2  4  2  0  0  2 13  2]\n",
      " [ 0  0  0  0  0  1  0  0  0  0 29]]\n",
      "Accuracy :\n",
      "0.6515151515151515\n",
      "F1 score :\n",
      "0.6457811829430375\n",
      "Precison :\n",
      "0.6637928827650845\n",
      "Recall :\n",
      "0.6515151515151515\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test, preds_test))\n",
    "print(\"Accuracy :\")\n",
    "print(accuracy_score(y_test, preds_test))\n",
    "print(\"F1 score :\")\n",
    "print(f1_score(y_test, preds_test, average='macro'))\n",
    "print(\"Precison :\")\n",
    "print(precision_score(y_test, preds_test, average='macro'))\n",
    "print(\"Recall :\")\n",
    "print(recall_score(y_test, preds_test, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takie wyniki, bez żadnej optymalizacji mogą być zadowalające, jednak na pewno nie wystarczające. Poniżej próba pierwszej optymalizacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}\n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 2000), #default value 100\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", -1, 2000), # default None\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 2000), # default 2\n",
    "            \"n_jobs\": trial.suggest_int(\"n_jobs\", -1, -1)\n",
    "        }\n",
    "    return space\n",
    "trials = 25\n",
    "\n",
    "def objective(trial, model, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=11), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-09 20:46:24,854]\u001b[0m A new study created in memory with name: no-name-794951d9-4d8c-4085-8552-909313822f31\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:46:47,627]\u001b[0m Trial 0 finished with value: 0.26600944611096816 and parameters: {'n_estimators': 925, 'max_depth': 1458, 'min_samples_split': 702, 'n_jobs': -1}. Best is trial 0 with value: 0.26600944611096816.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:47:08,850]\u001b[0m Trial 1 finished with value: 0.25500465729822347 and parameters: {'n_estimators': 983, 'max_depth': 866, 'min_samples_split': 830, 'n_jobs': -1}. Best is trial 0 with value: 0.26600944611096816.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:47:42,656]\u001b[0m Trial 2 finished with value: 0.013986013986013986 and parameters: {'n_estimators': 1868, 'max_depth': 618, 'min_samples_split': 1265, 'n_jobs': -1}. Best is trial 0 with value: 0.26600944611096816.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:47:59,622]\u001b[0m Trial 3 finished with value: 0.37073517927593486 and parameters: {'n_estimators': 710, 'max_depth': 1960, 'min_samples_split': 152, 'n_jobs': -1}. Best is trial 3 with value: 0.37073517927593486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:48:33,794]\u001b[0m Trial 4 finished with value: 0.013986013986013986 and parameters: {'n_estimators': 1939, 'max_depth': 482, 'min_samples_split': 1820, 'n_jobs': -1}. Best is trial 3 with value: 0.37073517927593486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:48:53,755]\u001b[0m Trial 5 finished with value: 0.013986013986013986 and parameters: {'n_estimators': 898, 'max_depth': 4, 'min_samples_split': 1749, 'n_jobs': -1}. Best is trial 3 with value: 0.37073517927593486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:49:10,324]\u001b[0m Trial 6 finished with value: 0.013986013986013986 and parameters: {'n_estimators': 624, 'max_depth': 1959, 'min_samples_split': 1208, 'n_jobs': -1}. Best is trial 3 with value: 0.37073517927593486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:49:37,698]\u001b[0m Trial 7 finished with value: 0.013986013986013986 and parameters: {'n_estimators': 1437, 'max_depth': 1547, 'min_samples_split': 1588, 'n_jobs': -1}. Best is trial 3 with value: 0.37073517927593486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:06,033]\u001b[0m Trial 8 finished with value: 0.30764049273818367 and parameters: {'n_estimators': 1279, 'max_depth': 88, 'min_samples_split': 438, 'n_jobs': -1}. Best is trial 3 with value: 0.37073517927593486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:07,662]\u001b[0m Trial 9 finished with value: 0.28490502249952615 and parameters: {'n_estimators': 38, 'max_depth': 446, 'min_samples_split': 444, 'n_jobs': -1}. Best is trial 3 with value: 0.37073517927593486.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:16,259]\u001b[0m Trial 10 finished with value: 0.3898647417407265 and parameters: {'n_estimators': 378, 'max_depth': 1948, 'min_samples_split': 99, 'n_jobs': -1}. Best is trial 10 with value: 0.3898647417407265.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:25,649]\u001b[0m Trial 11 finished with value: 0.508803703013512 and parameters: {'n_estimators': 327, 'max_depth': 1939, 'min_samples_split': 23, 'n_jobs': -1}. Best is trial 11 with value: 0.508803703013512.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:31,397]\u001b[0m Trial 12 finished with value: 0.5937491269294224 and parameters: {'n_estimators': 167, 'max_depth': 1457, 'min_samples_split': 10, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:33,259]\u001b[0m Trial 13 finished with value: 0.3152361363901479 and parameters: {'n_estimators': 53, 'max_depth': 1502, 'min_samples_split': 372, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:42,807]\u001b[0m Trial 14 finished with value: 0.44718733830976964 and parameters: {'n_estimators': 344, 'max_depth': 1325, 'min_samples_split': 56, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:48,118]\u001b[0m Trial 15 finished with value: 0.28132920754789814 and parameters: {'n_estimators': 244, 'max_depth': 1177, 'min_samples_split': 650, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:50:58,984]\u001b[0m Trial 16 finished with value: 0.33448104160876485 and parameters: {'n_estimators': 534, 'max_depth': 1678, 'min_samples_split': 280, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:51:23,580]\u001b[0m Trial 17 finished with value: 0.5262041639876118 and parameters: {'n_estimators': 1292, 'max_depth': 1744, 'min_samples_split': 21, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:51:44,459]\u001b[0m Trial 18 finished with value: 0.24426270414365006 and parameters: {'n_estimators': 1326, 'max_depth': 1053, 'min_samples_split': 1017, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:52:06,387]\u001b[0m Trial 19 finished with value: 0.28278360469054387 and parameters: {'n_estimators': 1570, 'max_depth': 1706, 'min_samples_split': 601, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:52:24,540]\u001b[0m Trial 20 finished with value: 0.34923690086473086 and parameters: {'n_estimators': 1150, 'max_depth': 1246, 'min_samples_split': 216, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:52:50,357]\u001b[0m Trial 21 finished with value: 0.47194198967110873 and parameters: {'n_estimators': 1687, 'max_depth': 1742, 'min_samples_split': 39, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:52:55,508]\u001b[0m Trial 22 finished with value: 0.3342540382405545 and parameters: {'n_estimators': 204, 'max_depth': 1832, 'min_samples_split': 288, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:53:11,306]\u001b[0m Trial 23 finished with value: 0.5937118909454656 and parameters: {'n_estimators': 764, 'max_depth': 1573, 'min_samples_split': 10, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:53:23,695]\u001b[0m Trial 24 finished with value: 0.303726718218385 and parameters: {'n_estimators': 758, 'max_depth': 1388, 'min_samples_split': 408, 'n_jobs': -1}. Best is trial 12 with value: 0.5937491269294224.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 42s\n",
      "Wall time: 6min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study01 = optuna.create_study(direction='maximize')\n",
    "study01.optimize(lambda x: objective(x, model, X_train_scaled, y_train), n_trials=trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej odpowiedni fit oraz wyświetlenie najlepszych parametrów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:  {'n_estimators': 167, 'max_depth': 1457, 'min_samples_split': 10, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "print('params: ', study01.best_params)\n",
    "\n",
    "lr = model(**study01.best_params)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "preds01 = lr.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniżej wyliczone metryki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[20  0  4  3  1  0  0  1  0  0  1]\n",
      " [ 1 11  5  5  3  1  0  0  0  0  4]\n",
      " [ 1  0 20  3  1  5  0  0  0  0  0]\n",
      " [ 1  0  1 20  0  0  4  0  0  1  3]\n",
      " [ 0  0  0  5 13  3  0  2  0  0  7]\n",
      " [ 0  0  0  1  1 23  1  0  0  0  4]\n",
      " [ 1  3  0  0  2  1 17  1  1  1  3]\n",
      " [ 3  2  2  1  1  1  3 11  4  1  1]\n",
      " [ 0  0  3  0  0  1  0  4 18  0  4]\n",
      " [ 1  2  2  2  3  4  0  0  0 13  3]\n",
      " [ 0  0  0  0  1  1  0  0  0  0 28]]\n",
      "Accuracy :\n",
      "0.5878787878787879\n",
      "F1 score :\n",
      "0.5805316631529825\n",
      "Precison :\n",
      "0.6161592773363862\n",
      "Recall :\n",
      "0.5878787878787879\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test, preds01))\n",
    "print(\"Accuracy :\")\n",
    "print(accuracy_score(y_test, preds01))\n",
    "print(\"F1 score :\")\n",
    "print(f1_score(y_test, preds01, average='macro'))\n",
    "print(\"Precison :\")\n",
    "print(precision_score(y_test, preds01, average='macro'))\n",
    "print(\"Recall :\")\n",
    "print(recall_score(y_test, preds01, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zgodnie z obserwacjami, rozszerzam zakres \"n_estimators\" oraz \"max_depth\" i zawężam \"min_samples_split\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}\n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 4000), #default value 100\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", -1, 4000), # default None\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 100), # default 2\n",
    "            \"n_jobs\": trial.suggest_int(\"n_jobs\", -1, -1)\n",
    "        }\n",
    "    return space\n",
    "trials = 25\n",
    "\n",
    "def objective(trial, model, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=11), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozpoczęcie drugiej optymalizacji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-09 20:53:24,261]\u001b[0m A new study created in memory with name: no-name-67291e11-759d-4332-a682-43f3136b9191\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:54:25,499]\u001b[0m Trial 0 finished with value: 0.41135991323860177 and parameters: {'n_estimators': 3793, 'max_depth': 1675, 'min_samples_split': 86, 'n_jobs': -1}. Best is trial 0 with value: 0.41135991323860177.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:54:36,722]\u001b[0m Trial 1 finished with value: 0.4535099897386195 and parameters: {'n_estimators': 549, 'max_depth': 1960, 'min_samples_split': 52, 'n_jobs': -1}. Best is trial 1 with value: 0.4535099897386195.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:55:15,426]\u001b[0m Trial 2 finished with value: 0.5479311491302412 and parameters: {'n_estimators': 2627, 'max_depth': 2954, 'min_samples_split': 17, 'n_jobs': -1}. Best is trial 2 with value: 0.5479311491302412.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:55:59,266]\u001b[0m Trial 3 finished with value: 0.4955199172498764 and parameters: {'n_estimators': 2608, 'max_depth': 2952, 'min_samples_split': 31, 'n_jobs': -1}. Best is trial 2 with value: 0.5479311491302412.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:56:50,761]\u001b[0m Trial 4 finished with value: 0.42389844422964823 and parameters: {'n_estimators': 3611, 'max_depth': 841, 'min_samples_split': 72, 'n_jobs': -1}. Best is trial 2 with value: 0.5479311491302412.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:57:43,738]\u001b[0m Trial 5 finished with value: 0.4182634397673338 and parameters: {'n_estimators': 2832, 'max_depth': 3673, 'min_samples_split': 77, 'n_jobs': -1}. Best is trial 2 with value: 0.5479311491302412.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 20:59:42,405]\u001b[0m Trial 6 finished with value: 0.6782085296596136 and parameters: {'n_estimators': 2430, 'max_depth': 2886, 'min_samples_split': 4, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:00:21,413]\u001b[0m Trial 7 finished with value: 0.4224134502295877 and parameters: {'n_estimators': 1584, 'max_depth': 3673, 'min_samples_split': 71, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:01:12,885]\u001b[0m Trial 8 finished with value: 0.5189402076327446 and parameters: {'n_estimators': 3587, 'max_depth': 435, 'min_samples_split': 23, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:01:28,778]\u001b[0m Trial 9 finished with value: 0.5116769022588636 and parameters: {'n_estimators': 1003, 'max_depth': 1109, 'min_samples_split': 24, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:02:11,238]\u001b[0m Trial 10 finished with value: 0.6638843380837922 and parameters: {'n_estimators': 1761, 'max_depth': 2797, 'min_samples_split': 5, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:03:09,854]\u001b[0m Trial 11 finished with value: 0.6475870643169898 and parameters: {'n_estimators': 1720, 'max_depth': 2700, 'min_samples_split': 6, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:04:41,459]\u001b[0m Trial 12 finished with value: 0.662048110212677 and parameters: {'n_estimators': 2082, 'max_depth': 2455, 'min_samples_split': 5, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:05:14,550]\u001b[0m Trial 13 finished with value: 0.4613707832286724 and parameters: {'n_estimators': 1219, 'max_depth': 3394, 'min_samples_split': 43, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:05:20,920]\u001b[0m Trial 14 finished with value: 0.6759546481447836 and parameters: {'n_estimators': 114, 'max_depth': 2305, 'min_samples_split': 2, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:05:29,176]\u001b[0m Trial 15 finished with value: 0.46423137296429606 and parameters: {'n_estimators': 261, 'max_depth': 2180, 'min_samples_split': 43, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:05:32,416]\u001b[0m Trial 16 finished with value: 0.542056789154774 and parameters: {'n_estimators': 61, 'max_depth': 1522, 'min_samples_split': 14, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:06:41,396]\u001b[0m Trial 17 finished with value: 0.4797775984445329 and parameters: {'n_estimators': 3017, 'max_depth': 3230, 'min_samples_split': 35, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:07:28,717]\u001b[0m Trial 18 finished with value: 0.45012433607851937 and parameters: {'n_estimators': 2262, 'max_depth': 3985, 'min_samples_split': 51, 'n_jobs': -1}. Best is trial 6 with value: 0.6782085296596136.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:08:07,327]\u001b[0m Trial 19 finished with value: 0.6888376948704397 and parameters: {'n_estimators': 880, 'max_depth': 2317, 'min_samples_split': 2, 'n_jobs': -1}. Best is trial 19 with value: 0.6888376948704397.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:08:32,318]\u001b[0m Trial 20 finished with value: 0.440107872665001 and parameters: {'n_estimators': 902, 'max_depth': 1523, 'min_samples_split': 59, 'n_jobs': -1}. Best is trial 19 with value: 0.6888376948704397.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:08:59,508]\u001b[0m Trial 21 finished with value: 0.6848548452067246 and parameters: {'n_estimators': 546, 'max_depth': 2273, 'min_samples_split': 2, 'n_jobs': -1}. Best is trial 19 with value: 0.6888376948704397.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:09:14,464]\u001b[0m Trial 22 finished with value: 0.5542539827733334 and parameters: {'n_estimators': 611, 'max_depth': 1946, 'min_samples_split': 15, 'n_jobs': -1}. Best is trial 19 with value: 0.6888376948704397.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:09:41,081]\u001b[0m Trial 23 finished with value: 0.4016461016297047 and parameters: {'n_estimators': 1295, 'max_depth': 2462, 'min_samples_split': 99, 'n_jobs': -1}. Best is trial 19 with value: 0.6888376948704397.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:09:51,291]\u001b[0m Trial 24 finished with value: 0.5736516156116545 and parameters: {'n_estimators': 508, 'max_depth': 2629, 'min_samples_split': 13, 'n_jobs': -1}. Best is trial 19 with value: 0.6888376948704397.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 7s\n",
      "Wall time: 16min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study02 = optuna.create_study(direction='maximize')\n",
    "study02.optimize(lambda x: objective(x, model, X_train_scaled, y_train), n_trials=trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wynik optymalizacji - parametry najlepszego modelu i predykcja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:  {'n_estimators': 880, 'max_depth': 2317, 'min_samples_split': 2, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "print('params: ', study02.best_params)\n",
    "\n",
    "lr = model(**study02.best_params)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "preds02 = lr.predict(X_test_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metryki oraz macierz pomyłek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[22  0  4  3  1  0  0  0  0  0  0]\n",
      " [ 2 14  4  4  1  0  0  0  0  0  5]\n",
      " [ 1  0 20  2  0  4  1  0  0  2  0]\n",
      " [ 1  0  1 23  0  0  4  0  0  1  0]\n",
      " [ 0  0  0  3 17  3  0  0  1  1  5]\n",
      " [ 0  0  0  0  0 27  1  0  0  0  2]\n",
      " [ 0  1  0  1  1  0 22  0  2  0  3]\n",
      " [ 3  0  1  2  1  1  3 13  5  0  1]\n",
      " [ 0  1  3  0  0  0  0  2 21  0  3]\n",
      " [ 3  2  2  2  2  2  0  0  1 14  2]\n",
      " [ 0  0  0  0  0  1  0  0  0  0 29]]\n",
      "Accuracy :\n",
      "0.6727272727272727\n",
      "F1 score :\n",
      "0.6644170811597889\n",
      "Precison :\n",
      "0.6995895421434286\n",
      "Recall :\n",
      "0.6727272727272727\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test, preds02))\n",
    "print(\"Accuracy :\")\n",
    "print(accuracy_score(y_test, preds02))\n",
    "print(\"F1 score :\")\n",
    "print(f1_score(y_test, preds02, average='macro'))\n",
    "print(\"Precison :\")\n",
    "print(precision_score(y_test, preds02, average='macro'))\n",
    "print(\"Recall :\")\n",
    "print(recall_score(y_test, preds02, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optymalizacja trzecia - dodanie parametrów \"criterion\", \"max_features\", \"random_state\", \"warm_start\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}         \n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 4500), #default value 100\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 5), # default 2\n",
    "            \"n_jobs\": trial.suggest_int(\"n_jobs\", -1, -1),\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"]), # default \"gini\"\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"]),\n",
    "            \"random_state\": trial.suggest_int(\"random_state\", 1, 50),\n",
    "            \"warm_start\": trial.suggest_categorical(\"warm_start\", [\"True\"])\n",
    "        }\n",
    "    return space\n",
    "trials = 25\n",
    "\n",
    "def objective(trial, model, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=11), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trzecia optymalizacja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-09 21:09:53,381]\u001b[0m A new study created in memory with name: no-name-fe0a9d5d-c86b-4088-921e-9dff16d74e43\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:12:08,395]\u001b[0m Trial 0 finished with value: 0.6907374124310103 and parameters: {'n_estimators': 2278, 'min_samples_split': 2, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt', 'random_state': 17, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:13:57,282]\u001b[0m Trial 1 finished with value: 0.6594156473287422 and parameters: {'n_estimators': 2143, 'min_samples_split': 5, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'log2', 'random_state': 28, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:14:26,253]\u001b[0m Trial 2 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:15:41,050]\u001b[0m Trial 3 finished with value: 0.6891529597001714 and parameters: {'n_estimators': 1187, 'min_samples_split': 3, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt', 'random_state': 29, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:19:50,285]\u001b[0m Trial 4 finished with value: 0.682625534551759 and parameters: {'n_estimators': 3480, 'min_samples_split': 3, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2', 'random_state': 24, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:21:43,811]\u001b[0m Trial 5 finished with value: 0.6621448225321417 and parameters: {'n_estimators': 1616, 'min_samples_split': 5, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'log2', 'random_state': 21, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:22:19,675]\u001b[0m Trial 6 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:22:47,048]\u001b[0m Trial 7 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:23:18,726]\u001b[0m Trial 8 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:23:50,148]\u001b[0m Trial 9 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:24:53,230]\u001b[0m Trial 10 finished with value: 0.6522895523273508 and parameters: {'n_estimators': 1322, 'min_samples_split': 5, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'log2', 'random_state': 45, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:26:19,163]\u001b[0m Trial 11 finished with value: 0.6592431991535492 and parameters: {'n_estimators': 1735, 'min_samples_split': 5, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'auto', 'random_state': 45, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:26:50,627]\u001b[0m Trial 12 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:27:21,673]\u001b[0m Trial 13 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:29:32,409]\u001b[0m Trial 14 finished with value: 0.6699541090479856 and parameters: {'n_estimators': 2177, 'min_samples_split': 4, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2', 'random_state': 26, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:30:01,194]\u001b[0m Trial 15 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:30:43,364]\u001b[0m Trial 16 finished with value: 0.6829238230395006 and parameters: {'n_estimators': 761, 'min_samples_split': 2, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'auto', 'random_state': 35, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:31:10,546]\u001b[0m Trial 17 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:32:37,332]\u001b[0m Trial 18 finished with value: 0.6593846678718994 and parameters: {'n_estimators': 1935, 'min_samples_split': 5, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'log2', 'random_state': 35, 'warm_start': 'True'}. Best is trial 0 with value: 0.6907374124310103.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:33:08,236]\u001b[0m Trial 19 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:33:41,224]\u001b[0m Trial 20 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:34:14,290]\u001b[0m Trial 21 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:34:47,938]\u001b[0m Trial 22 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:35:23,128]\u001b[0m Trial 23 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "11 fits failed out of a total of 11.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "11 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:35:58,465]\u001b[0m Trial 24 failed because of the following error: The value nan is not acceptable.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 54s\n",
      "Wall time: 26min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study03 = optuna.create_study(direction='maximize')\n",
    "study03.optimize(lambda x: objective(x, model, X_train_scaled, y_train), n_trials=trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predykcje na zbiorze testowym, wyniki i najlepszy model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:  {'n_estimators': 2278, 'min_samples_split': 2, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt', 'random_state': 17, 'warm_start': 'True'}\n"
     ]
    }
   ],
   "source": [
    "print('params: ', study03.best_params)\n",
    "\n",
    "lr = model(**study03.best_params)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "preds03 = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[22  0  4  3  1  0  0  0  0  0  0]\n",
      " [ 1 15  5  3  1  0  0  0  0  0  5]\n",
      " [ 1  0 19  2  1  4  2  0  0  1  0]\n",
      " [ 1  1  1 23  0  0  4  0  0  0  0]\n",
      " [ 0  0  0  3 18  2  0  0  1  1  5]\n",
      " [ 0  0  0  0  0 27  1  0  0  0  2]\n",
      " [ 0  1  0  1  0  0 22  0  1  0  5]\n",
      " [ 5  0  1  2  1  0  3 15  3  0  0]\n",
      " [ 0  0  3  0  0  0  0  2 22  0  3]\n",
      " [ 3  1  2  2  2  1  0  0  1 15  3]\n",
      " [ 0  0  0  0  0  1  0  0  0  0 29]]\n",
      "Accuracy :\n",
      "0.6878787878787879\n",
      "F1 score :\n",
      "0.6840310521811128\n",
      "Precison :\n",
      "0.7226947072535308\n",
      "Recall :\n",
      "0.687878787878788\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test, preds03))\n",
    "print(\"Accuracy :\")\n",
    "print(accuracy_score(y_test, preds03))\n",
    "print(\"F1 score :\")\n",
    "print(f1_score(y_test, preds03, average='macro'))\n",
    "print(\"Precison :\")\n",
    "print(precision_score(y_test, preds03, average='macro'))\n",
    "print(\"Recall :\")\n",
    "print(recall_score(y_test, preds03, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czwarta optymalizacja - weryfikacja zmiany scorer'a na accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier\n",
    "scoring = {'accuracy': make_scorer(accuracy_score)}         \n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 4000), #default value 100\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 2000), # default 2\n",
    "            \"n_jobs\": trial.suggest_int(\"n_jobs\", -1, -1)\n",
    "        }\n",
    "    return space\n",
    "trials = 25\n",
    "\n",
    "def objective(trial, model, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=11), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czwarty study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-09 21:36:05,565]\u001b[0m A new study created in memory with name: no-name-62b890e4-f505-4cb1-8908-f8ab26893d9a\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:37:05,138]\u001b[0m Trial 0 finished with value: 0.33636363636363636 and parameters: {'n_estimators': 2001, 'min_samples_split': 698, 'n_jobs': -1}. Best is trial 0 with value: 0.33636363636363636.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:37:25,960]\u001b[0m Trial 1 finished with value: 0.08333333333333334 and parameters: {'n_estimators': 581, 'min_samples_split': 1752, 'n_jobs': -1}. Best is trial 0 with value: 0.33636363636363636.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:38:05,122]\u001b[0m Trial 2 finished with value: 0.5242424242424243 and parameters: {'n_estimators': 1065, 'min_samples_split': 29, 'n_jobs': -1}. Best is trial 2 with value: 0.5242424242424243.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:38:27,137]\u001b[0m Trial 3 finished with value: 0.3530303030303031 and parameters: {'n_estimators': 564, 'min_samples_split': 509, 'n_jobs': -1}. Best is trial 2 with value: 0.5242424242424243.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:38:56,119]\u001b[0m Trial 4 finished with value: 0.08333333333333334 and parameters: {'n_estimators': 839, 'min_samples_split': 1535, 'n_jobs': -1}. Best is trial 2 with value: 0.5242424242424243.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:40:33,818]\u001b[0m Trial 5 finished with value: 0.32196969696969696 and parameters: {'n_estimators': 3510, 'min_samples_split': 800, 'n_jobs': -1}. Best is trial 2 with value: 0.5242424242424243.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:41:11,805]\u001b[0m Trial 6 finished with value: 0.32651515151515154 and parameters: {'n_estimators': 1182, 'min_samples_split': 815, 'n_jobs': -1}. Best is trial 2 with value: 0.5242424242424243.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:41:21,749]\u001b[0m Trial 7 finished with value: 0.08333333333333334 and parameters: {'n_estimators': 213, 'min_samples_split': 1978, 'n_jobs': -1}. Best is trial 2 with value: 0.5242424242424243.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:41:34,536]\u001b[0m Trial 8 finished with value: 0.08333333333333334 and parameters: {'n_estimators': 290, 'min_samples_split': 1985, 'n_jobs': -1}. Best is trial 2 with value: 0.5242424242424243.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:43:17,102]\u001b[0m Trial 9 finished with value: 0.31742424242424244 and parameters: {'n_estimators': 3644, 'min_samples_split': 1014, 'n_jobs': -1}. Best is trial 2 with value: 0.5242424242424243.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:44:01,148]\u001b[0m Trial 10 finished with value: 0.6196969696969696 and parameters: {'n_estimators': 1794, 'min_samples_split': 9, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:44:34,031]\u001b[0m Trial 11 finished with value: 0.5636363636363636 and parameters: {'n_estimators': 1829, 'min_samples_split': 18, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:45:11,488]\u001b[0m Trial 12 finished with value: 0.4969696969696969 and parameters: {'n_estimators': 2285, 'min_samples_split': 44, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:45:41,326]\u001b[0m Trial 13 finished with value: 0.3871212121212121 and parameters: {'n_estimators': 1964, 'min_samples_split': 271, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:46:45,546]\u001b[0m Trial 14 finished with value: 0.3772727272727273 and parameters: {'n_estimators': 2702, 'min_samples_split': 371, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:47:33,160]\u001b[0m Trial 15 finished with value: 0.08333333333333334 and parameters: {'n_estimators': 1476, 'min_samples_split': 1232, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:48:57,300]\u001b[0m Trial 16 finished with value: 0.3909090909090909 and parameters: {'n_estimators': 2846, 'min_samples_split': 242, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:49:52,060]\u001b[0m Trial 17 finished with value: 0.3530303030303031 and parameters: {'n_estimators': 1634, 'min_samples_split': 562, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:51:09,893]\u001b[0m Trial 18 finished with value: 0.30757575757575756 and parameters: {'n_estimators': 2652, 'min_samples_split': 1190, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:52:06,664]\u001b[0m Trial 19 finished with value: 0.41212121212121217 and parameters: {'n_estimators': 1630, 'min_samples_split': 159, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:53:44,694]\u001b[0m Trial 20 finished with value: 0.36742424242424243 and parameters: {'n_estimators': 3164, 'min_samples_split': 424, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:54:17,892]\u001b[0m Trial 21 finished with value: 0.5446969696969697 and parameters: {'n_estimators': 1179, 'min_samples_split': 23, 'n_jobs': -1}. Best is trial 10 with value: 0.6196969696969696.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:55:21,345]\u001b[0m Trial 22 finished with value: 0.6439393939393939 and parameters: {'n_estimators': 2280, 'min_samples_split': 7, 'n_jobs': -1}. Best is trial 22 with value: 0.6439393939393939.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:56:03,527]\u001b[0m Trial 23 finished with value: 0.3939393939393939 and parameters: {'n_estimators': 2238, 'min_samples_split': 221, 'n_jobs': -1}. Best is trial 22 with value: 0.6439393939393939.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:56:47,886]\u001b[0m Trial 24 finished with value: 0.4136363636363637 and parameters: {'n_estimators': 2382, 'min_samples_split': 151, 'n_jobs': -1}. Best is trial 22 with value: 0.6439393939393939.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 46s\n",
      "Wall time: 20min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study04 = optuna.create_study(direction='maximize')\n",
    "study04.optimize(lambda x: objective(x, model, X_train_scaled, y_train), n_trials=trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki i metryki - znaczny spadek skuteczności w przypadku zmiany scorer'a:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:  {'n_estimators': 2280, 'min_samples_split': 7, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "print('params: ', study04.best_params)\n",
    "\n",
    "lr = model(**study04.best_params)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "preds04 = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[21  0  4  3  1  0  0  0  0  0  1]\n",
      " [ 2 14  4  4  1  0  0  0  0  0  5]\n",
      " [ 1  0 19  2  1  4  2  0  0  1  0]\n",
      " [ 1  0  1 22  0  0  4  0  0  1  1]\n",
      " [ 0  0  0  4 16  2  0  1  1  1  5]\n",
      " [ 0  0  0  0  0 25  1  0  0  2  2]\n",
      " [ 1  0  0  1  2  0 18  1  2  1  4]\n",
      " [ 3  0  1  3  1  1  4 11  4  1  1]\n",
      " [ 0  0  3  0  0  0  1  3 18  0  5]\n",
      " [ 2  1  2  3  2  2  0  0  2 14  2]\n",
      " [ 0  0  0  0  1  1  0  0  0  0 28]]\n",
      "Accuracy :\n",
      "0.6242424242424243\n",
      "F1 score :\n",
      "0.6174711787263328\n",
      "Precison :\n",
      "0.6533657552300817\n",
      "Recall :\n",
      "0.6242424242424242\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test, preds04))\n",
    "print(\"Accuracy :\")\n",
    "print(accuracy_score(y_test, preds04))\n",
    "print(\"F1 score :\")\n",
    "print(f1_score(y_test, preds04, average='macro'))\n",
    "print(\"Precison :\")\n",
    "print(precision_score(y_test, preds04, average='macro'))\n",
    "print(\"Recall :\")\n",
    "print(recall_score(y_test, preds04, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optymalizacja piąta - zbadanie wpłwu \"foldów\" w crosswalidacji na wyniki:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}         \n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 4500), #default value 100\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 5), # default 2\n",
    "            \"n_jobs\": trial.suggest_int(\"n_jobs\", -1, -1),\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"]), # default \"gini\"\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\", \"log2\"]),\n",
    "            \"random_state\": trial.suggest_int(\"random_state\", 1, 50),\n",
    "            \"warm_start\": trial.suggest_categorical(\"warm_start\", [\"True\"])\n",
    "        }\n",
    "    return space\n",
    "trials = 25\n",
    "\n",
    "def objective(trial, model, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=5), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optymalizacja piąta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-09 21:56:54,247]\u001b[0m A new study created in memory with name: no-name-3828dec8-694a-4887-8582-b66b13bba51f\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:57:34,488]\u001b[0m Trial 0 finished with value: 0.6371119877109087 and parameters: {'n_estimators': 1937, 'min_samples_split': 5, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'auto', 'random_state': 26, 'warm_start': 'True'}. Best is trial 0 with value: 0.6371119877109087.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 21:58:07,517]\u001b[0m Trial 1 finished with value: 0.6375257107939358 and parameters: {'n_estimators': 1624, 'min_samples_split': 5, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'auto', 'random_state': 16, 'warm_start': 'True'}. Best is trial 1 with value: 0.6375257107939358.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:58:22,908]\u001b[0m Trial 2 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 21:58:36,783]\u001b[0m Trial 3 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:00:23,091]\u001b[0m Trial 4 finished with value: 0.640748449119364 and parameters: {'n_estimators': 3650, 'min_samples_split': 4, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2', 'random_state': 13, 'warm_start': 'True'}. Best is trial 4 with value: 0.640748449119364.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:02:07,521]\u001b[0m Trial 5 finished with value: 0.671125965571909 and parameters: {'n_estimators': 2837, 'min_samples_split': 2, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'auto', 'random_state': 24, 'warm_start': 'True'}. Best is trial 5 with value: 0.671125965571909.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:03:42,189]\u001b[0m Trial 6 finished with value: 0.6623413065201122 and parameters: {'n_estimators': 2787, 'min_samples_split': 2, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'log2', 'random_state': 19, 'warm_start': 'True'}. Best is trial 5 with value: 0.671125965571909.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:03:55,778]\u001b[0m Trial 7 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:04:14,399]\u001b[0m Trial 8 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:06:26,386]\u001b[0m Trial 9 finished with value: 0.6586303768475459 and parameters: {'n_estimators': 3981, 'min_samples_split': 3, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'auto', 'random_state': 45, 'warm_start': 'True'}. Best is trial 5 with value: 0.671125965571909.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:06:36,543]\u001b[0m Trial 10 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:06:52,212]\u001b[0m Trial 11 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:08:19,271]\u001b[0m Trial 12 finished with value: 0.6375605379174352 and parameters: {'n_estimators': 2813, 'min_samples_split': 5, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'log2', 'random_state': 26, 'warm_start': 'True'}. Best is trial 5 with value: 0.671125965571909.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:09:27,560]\u001b[0m Trial 13 finished with value: 0.6447094870386019 and parameters: {'n_estimators': 1912, 'min_samples_split': 4, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2', 'random_state': 29, 'warm_start': 'True'}. Best is trial 5 with value: 0.671125965571909.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:11:48,734]\u001b[0m Trial 14 finished with value: 0.6496134711722096 and parameters: {'n_estimators': 4019, 'min_samples_split': 4, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'auto', 'random_state': 19, 'warm_start': 'True'}. Best is trial 5 with value: 0.671125965571909.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:12:40,976]\u001b[0m Trial 15 finished with value: 0.6632254313366667 and parameters: {'n_estimators': 1517, 'min_samples_split': 2, 'n_jobs': -1, 'criterion': 'gini', 'max_features': 'log2', 'random_state': 9, 'warm_start': 'True'}. Best is trial 5 with value: 0.671125965571909.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:12:50,348]\u001b[0m Trial 16 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:13:02,583]\u001b[0m Trial 17 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:13:14,465]\u001b[0m Trial 18 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:13:26,714]\u001b[0m Trial 19 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:13:39,225]\u001b[0m Trial 20 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:13:51,884]\u001b[0m Trial 21 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:14:05,616]\u001b[0m Trial 22 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:14:19,430]\u001b[0m Trial 23 failed because of the following error: The value nan is not acceptable.\u001b[0m\n",
      "d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "joblib.externals.loky.process_executor._RemoteTraceback: \n",
      "\"\"\"\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n",
      "    r = call_item()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n",
      "    return self.fn(*self.args, **self.kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 187, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=sample_weight, check_input=False)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 352, in fit\n",
      "    criterion = CRITERIA_CLF[self.criterion](\n",
      "KeyError: 'log_loss'\n",
      "\"\"\"\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 542, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 446, in result\n",
      "    return self.__get_result()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\concurrent\\futures\\_base.py\", line 391, in __get_result\n",
      "    raise self._exception\n",
      "KeyError: 'log_loss'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "\u001b[33m[W 2023-01-09 22:14:33,037]\u001b[0m Trial 24 failed because of the following error: The value nan is not acceptable.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 57s\n",
      "Wall time: 17min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study05 = optuna.create_study(direction='maximize')\n",
    "study05.optimize(lambda x: objective(x, model, X_train_scaled, y_train), n_trials=trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki piątej optymalizacji - w tym wypadku 5 foldów sprawdziło się lepiej [całościowo najlepszy ze wszystkich na zbiorze testowym]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:  {'n_estimators': 2837, 'min_samples_split': 2, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'auto', 'random_state': 24, 'warm_start': 'True'}\n"
     ]
    }
   ],
   "source": [
    "print('params: ', study05.best_params)\n",
    "\n",
    "lr = model(**study05.best_params)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "preds05 = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[22  0  4  3  1  0  0  0  0  0  0]\n",
      " [ 1 15  5  3  1  0  0  0  0  0  5]\n",
      " [ 1  0 20  2  0  4  1  0  0  2  0]\n",
      " [ 1  0  1 23  0  0  4  0  0  1  0]\n",
      " [ 0  0  0  4 19  1  0  0  0  1  5]\n",
      " [ 0  0  0  0  0 28  1  0  0  0  1]\n",
      " [ 0  1  0  1  0  0 22  1  2  0  3]\n",
      " [ 4  0  1  2  1  1  3 15  3  0  0]\n",
      " [ 0  0  3  0  0  0  1  2 20  0  4]\n",
      " [ 2  1  2  3  1  2  0  0  1 15  3]\n",
      " [ 0  0  0  0  0  1  0  0  0  0 29]]\n",
      "Accuracy :\n",
      "0.6909090909090909\n",
      "F1 score :\n",
      "0.6865970306320115\n",
      "Precison :\n",
      "0.7228130023541898\n",
      "Recall :\n",
      "0.6909090909090909\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test, preds05))\n",
    "print(\"Accuracy :\")\n",
    "print(accuracy_score(y_test, preds05))\n",
    "print(\"F1 score :\")\n",
    "print(f1_score(y_test, preds05, average='macro'))\n",
    "print(\"Precison :\")\n",
    "print(precision_score(y_test, preds05, average='macro'))\n",
    "print(\"Recall :\")\n",
    "print(recall_score(y_test, preds05, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finałowa, ostatnia optymalizacja:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtraTreesClassifier\n",
    "scoring = {'f1_macro': make_scorer(f1_score, average='macro')}         \n",
    "\n",
    "def get_space(trial): \n",
    "    space = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 2, 2500), #default value 100\n",
    "            \"n_jobs\": trial.suggest_int(\"n_jobs\", -1, -1),\n",
    "            \"criterion\": trial.suggest_categorical(\"criterion\", [\"entropy\"]), # default \"gini\"\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "        }\n",
    "    return space\n",
    "trials = 50\n",
    "\n",
    "def objective(trial, model, X, y):\n",
    "    model_space = get_space(trial)\n",
    "\n",
    "    mdl = model(**model_space)\n",
    "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=11), return_train_score=True)\n",
    "\n",
    "    return np.mean(scores['test_f1_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-01-09 22:17:04,742]\u001b[0m A new study created in memory with name: no-name-79e9b520-786a-466f-bbd9-3d794aa0fa0a\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:21:00,408]\u001b[0m Trial 0 finished with value: 0.6858923102054312 and parameters: {'n_estimators': 1897, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2'}. Best is trial 0 with value: 0.6858923102054312.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:21:34,142]\u001b[0m Trial 1 finished with value: 0.6810537883936397 and parameters: {'n_estimators': 337, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2'}. Best is trial 0 with value: 0.6858923102054312.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:26:08,427]\u001b[0m Trial 2 finished with value: 0.6830479250268031 and parameters: {'n_estimators': 1810, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2'}. Best is trial 0 with value: 0.6858923102054312.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:32:10,944]\u001b[0m Trial 3 finished with value: 0.692547271118574 and parameters: {'n_estimators': 2315, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2'}. Best is trial 3 with value: 0.692547271118574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:37:38,209]\u001b[0m Trial 4 finished with value: 0.6857767510261326 and parameters: {'n_estimators': 2002, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2'}. Best is trial 3 with value: 0.692547271118574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:41:58,926]\u001b[0m Trial 5 finished with value: 0.691723743058714 and parameters: {'n_estimators': 1565, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 3 with value: 0.692547271118574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:46:27,353]\u001b[0m Trial 6 finished with value: 0.6873230061337006 and parameters: {'n_estimators': 1535, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 3 with value: 0.692547271118574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:50:31,289]\u001b[0m Trial 7 finished with value: 0.6905906698854971 and parameters: {'n_estimators': 1397, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2'}. Best is trial 3 with value: 0.692547271118574.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:52:41,053]\u001b[0m Trial 8 finished with value: 0.7002028225766752 and parameters: {'n_estimators': 733, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:55:47,958]\u001b[0m Trial 9 finished with value: 0.6874862938066986 and parameters: {'n_estimators': 922, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'log2'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 22:56:14,909]\u001b[0m Trial 10 finished with value: 0.6698705697940246 and parameters: {'n_estimators': 136, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:02:14,020]\u001b[0m Trial 11 finished with value: 0.693472620629282 and parameters: {'n_estimators': 2386, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:04:09,659]\u001b[0m Trial 12 finished with value: 0.6933623885049042 and parameters: {'n_estimators': 817, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:06:04,945]\u001b[0m Trial 13 finished with value: 0.688159200293991 and parameters: {'n_estimators': 831, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:11:52,243]\u001b[0m Trial 14 finished with value: 0.6928282356278475 and parameters: {'n_estimators': 2339, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:12:53,256]\u001b[0m Trial 15 finished with value: 0.675568066272964 and parameters: {'n_estimators': 547, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:15:55,035]\u001b[0m Trial 16 finished with value: 0.692070398938654 and parameters: {'n_estimators': 1232, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:18:45,760]\u001b[0m Trial 17 finished with value: 0.6896449554614882 and parameters: {'n_estimators': 1114, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:20:27,455]\u001b[0m Trial 18 finished with value: 0.6913919732518333 and parameters: {'n_estimators': 562, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:20:39,873]\u001b[0m Trial 19 finished with value: 0.6487059963085308 and parameters: {'n_estimators': 59, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:26:58,040]\u001b[0m Trial 20 finished with value: 0.6921050937609272 and parameters: {'n_estimators': 2070, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:29:08,585]\u001b[0m Trial 21 finished with value: 0.6871516207478383 and parameters: {'n_estimators': 793, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:32:07,113]\u001b[0m Trial 22 finished with value: 0.6861229475776371 and parameters: {'n_estimators': 1013, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:34:00,805]\u001b[0m Trial 23 finished with value: 0.6811365261725656 and parameters: {'n_estimators': 649, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:34:47,234]\u001b[0m Trial 24 finished with value: 0.6840081530354222 and parameters: {'n_estimators': 284, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:37:53,515]\u001b[0m Trial 25 finished with value: 0.6912833478241575 and parameters: {'n_estimators': 1230, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:38:36,463]\u001b[0m Trial 26 finished with value: 0.685570328393156 and parameters: {'n_estimators': 404, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:39:37,420]\u001b[0m Trial 27 finished with value: 0.6887219638755 and parameters: {'n_estimators': 677, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:42:15,036]\u001b[0m Trial 28 finished with value: 0.6905028520818791 and parameters: {'n_estimators': 1675, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:44:40,356]\u001b[0m Trial 29 finished with value: 0.6903008893154753 and parameters: {'n_estimators': 1407, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:46:33,396]\u001b[0m Trial 30 finished with value: 0.6932779278940209 and parameters: {'n_estimators': 1074, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:48:58,465]\u001b[0m Trial 31 finished with value: 0.6871167085753833 and parameters: {'n_estimators': 1114, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:51:10,738]\u001b[0m Trial 32 finished with value: 0.6945245335348098 and parameters: {'n_estimators': 924, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[32m[I 2023-01-09 23:53:13,839]\u001b[0m Trial 33 finished with value: 0.6956453845237275 and parameters: {'n_estimators': 837, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}. Best is trial 8 with value: 0.7002028225766752.\u001b[0m\n",
      "\u001b[33m[W 2023-01-09 23:53:42,214]\u001b[0m Trial 34 failed because of the following error: KeyboardInterrupt()\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"<timed exec>\", line 2, in <lambda>\n",
      "  File \"C:\\Users\\kubag\\AppData\\Local\\Temp\\ipykernel_3600\\2821399092.py\", line 18, in objective\n",
      "    scores = cross_validate(mdl, X, y, scoring=scoring, cv=StratifiedKFold(n_splits=11), return_train_score=True)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 267, in cross_validate\n",
      "    results = parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1046, in __call__\n",
      "    while self.dispatch_one_batch(iterator):\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 705, in _fit_and_score\n",
      "    train_scores = _score(estimator, X_train, y_train, scorer, error_score)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 761, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 103, in __call__\n",
      "    score = scorer._score(cached_call, estimator, *args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 258, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 68, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 808, in predict\n",
      "    proba = self.predict_proba(X)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 861, in predict_proba\n",
      "    Parallel(\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 1056, in __call__\n",
      "    self.retrieve()\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py\", line 935, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\multiprocessing\\pool.py\", line 765, in get\n",
      "    self.wait(timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\multiprocessing\\pool.py\", line 762, in wait\n",
      "    self._event.wait(timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\threading.py\", line 581, in wait\n",
      "    signaled = self._cond.wait(timeout)\n",
      "  File \"d:\\Anaconda\\envs\\ML_P01\\lib\\threading.py\", line 312, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:2\u001b[0m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\optuna\\study\\study.py:419\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    316\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    317\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    324\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    325\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m     \u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \n\u001b[0;32m    328\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m     _optimize(\n\u001b[0;32m    420\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    421\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    422\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    423\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    424\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    425\u001b[0m         catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    426\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    427\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    428\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    161\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as CircleCI).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\optuna\\study\\_optimize.py:234\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    230\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    231\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    232\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    233\u001b[0m ):\n\u001b[1;32m--> 234\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    235\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    197\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "File \u001b[1;32m<timed exec>:2\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n",
      "Cell \u001b[1;32mIn[134], line 18\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial, model, X, y)\u001b[0m\n\u001b[0;32m     15\u001b[0m model_space \u001b[39m=\u001b[39m get_space(trial)\n\u001b[0;32m     17\u001b[0m mdl \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_space)\n\u001b[1;32m---> 18\u001b[0m scores \u001b[39m=\u001b[39m cross_validate(mdl, X, y, scoring\u001b[39m=\u001b[39;49mscoring, cv\u001b[39m=\u001b[39;49mStratifiedKFold(n_splits\u001b[39m=\u001b[39;49m\u001b[39m11\u001b[39;49m), return_train_score\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(scores[\u001b[39m'\u001b[39m\u001b[39mtest_f1_macro\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:267\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 267\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    268\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    269\u001b[0m         clone(estimator),\n\u001b[0;32m    270\u001b[0m         X,\n\u001b[0;32m    271\u001b[0m         y,\n\u001b[0;32m    272\u001b[0m         scorers,\n\u001b[0;32m    273\u001b[0m         train,\n\u001b[0;32m    274\u001b[0m         test,\n\u001b[0;32m    275\u001b[0m         verbose,\n\u001b[0;32m    276\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    277\u001b[0m         fit_params,\n\u001b[0;32m    278\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[0;32m    279\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    280\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[0;32m    281\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m cv\u001b[39m.\u001b[39;49msplit(X, y, groups)\n\u001b[0;32m    284\u001b[0m )\n\u001b[0;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[0;32m    288\u001b[0m \u001b[39m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1047\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:705\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    703\u001b[0m     score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[0;32m    704\u001b[0m     \u001b[39mif\u001b[39;00m return_train_score:\n\u001b[1;32m--> 705\u001b[0m         train_scores \u001b[39m=\u001b[39m _score(estimator, X_train, y_train, scorer, error_score)\n\u001b[0;32m    707\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    708\u001b[0m     total_time \u001b[39m=\u001b[39m score_time \u001b[39m+\u001b[39m fit_time\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:761\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    759\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test)\n\u001b[0;32m    760\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 761\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[0;32m    762\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    763\u001b[0m     \u001b[39mif\u001b[39;00m error_score \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:103\u001b[0m, in \u001b[0;36m_MultimetricScorer.__call__\u001b[1;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mfor\u001b[39;00m name, scorer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scorers\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(scorer, _BaseScorer):\n\u001b[1;32m--> 103\u001b[0m         score \u001b[39m=\u001b[39m scorer\u001b[39m.\u001b[39m_score(cached_call, estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    104\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m         score \u001b[39m=\u001b[39m scorer(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:258\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_score\u001b[39m(\u001b[39mself\u001b[39m, method_caller, estimator, X, y_true, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    231\u001b[0m     \u001b[39m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \n\u001b[0;32m    233\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39m        Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     y_pred \u001b[39m=\u001b[39m method_caller(estimator, \u001b[39m\"\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m\"\u001b[39;49m, X)\n\u001b[0;32m    259\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    260\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sign \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score_func(\n\u001b[0;32m    261\u001b[0m             y_true, y_pred, sample_weight\u001b[39m=\u001b[39msample_weight, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs\n\u001b[0;32m    262\u001b[0m         )\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:68\u001b[0m, in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(estimator, method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     70\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m cache[method]\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:808\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    788\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[0;32m    790\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[0;32m    807\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 808\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[0;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    811\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:861\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    856\u001b[0m all_proba \u001b[39m=\u001b[39m [\n\u001b[0;32m    857\u001b[0m     np\u001b[39m.\u001b[39mzeros((X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], j), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m    858\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39matleast_1d(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_)\n\u001b[0;32m    859\u001b[0m ]\n\u001b[0;32m    860\u001b[0m lock \u001b[39m=\u001b[39m threading\u001b[39m.\u001b[39mLock()\n\u001b[1;32m--> 861\u001b[0m Parallel(\n\u001b[0;32m    862\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    863\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    864\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(require\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msharedmem\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    865\u001b[0m )(\n\u001b[0;32m    866\u001b[0m     delayed(_accumulate_prediction)(e\u001b[39m.\u001b[39;49mpredict_proba, X, all_proba, lock)\n\u001b[0;32m    867\u001b[0m     \u001b[39mfor\u001b[39;49;00m e \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimators_\n\u001b[0;32m    868\u001b[0m )\n\u001b[0;32m    870\u001b[0m \u001b[39mfor\u001b[39;00m proba \u001b[39min\u001b[39;00m all_proba:\n\u001b[0;32m    871\u001b[0m     proba \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\multiprocessing\\pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[0;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\multiprocessing\\pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[0;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\ML_P01\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "study06 = optuna.create_study(direction='maximize')\n",
    "study06.optimize(lambda x: objective(x, model, X_train_scaled, y_train), n_trials=trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyniki predykcji oraz parametry/metryki [najlepszy wynik na zbiorze treningowym]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:  {'n_estimators': 733, 'n_jobs': -1, 'criterion': 'entropy', 'max_features': 'sqrt'}\n"
     ]
    }
   ],
   "source": [
    "print('params: ', study06.best_params)\n",
    "\n",
    "lr = model(**study06.best_params)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "preds06 = lr.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix :\n",
      "[[22  0  4  3  1  0  0  0  0  0  0]\n",
      " [ 1 15  5  3  1  0  0  0  0  0  5]\n",
      " [ 1  0 20  2  0  4  1  0  0  2  0]\n",
      " [ 1  1  1 22  0  0  4  0  0  1  0]\n",
      " [ 0  0  0  4 17  2  0  1  1  1  4]\n",
      " [ 0  0  0  0  0 26  1  0  0  1  2]\n",
      " [ 0  0  0  1  1  0 23  0  2  1  2]\n",
      " [ 6  2  0  1  1  0  2 13  3  1  1]\n",
      " [ 0  0  3  0  0  0  1  1 22  0  3]\n",
      " [ 2  1  2  3  2  2  0  0  1 15  2]\n",
      " [ 0  0  0  0  0  1  0  0  0  0 29]]\n",
      "Accuracy :\n",
      "0.6787878787878788\n",
      "F1 score :\n",
      "0.6719292362877295\n",
      "Precison :\n",
      "0.700334660804979\n",
      "Recall :\n",
      "0.6787878787878788\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion matrix :\")\n",
    "print(confusion_matrix(y_test, preds06))\n",
    "print(\"Accuracy :\")\n",
    "print(accuracy_score(y_test, preds06))\n",
    "print(\"F1 score :\")\n",
    "print(f1_score(y_test, preds06, average='macro'))\n",
    "print(\"Precison :\")\n",
    "print(precision_score(y_test, preds06, average='macro'))\n",
    "print(\"Recall :\")\n",
    "print(recall_score(y_test, preds06, average='macro'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omówienie w raporcie."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z podziękowaniami za możliwość darmowego użycia bazy IRMAS:\n",
    "\n",
    "Bosch, J. J., Janer, J., Fuhrmann, F., & Herrera, P. “A Comparison of Sound Segregation Techniques for Predominant Instrument Recognition in Musical Audio Signals”, in Proc. ISMIR (pp. 559-564), 2012\n",
    "\n",
    "The creation of this dataset was partially supported by “La Caixa” Fellowship Program, and the following projects: Classical Planet: TSI-070100-2009-407 (MITYC), DRIMS: TIN2009-14247-C02-01 (MICINN) and MIRES: EC-FP7 ICT-2011.1.5 Networked Media and Search Systems, grant agreement No. 287711. Additionally supported by TECNIO network promoted by ACC1Ó agency by the Catalan Government."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d4ce8c6234df13d4fb846774aa8eccab49798db3f3158354c0dd27f5c6d9c7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
